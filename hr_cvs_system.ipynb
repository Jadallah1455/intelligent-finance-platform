{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26623,
     "status": "ok",
     "timestamp": 1760736063621,
     "user": {
      "displayName": "leena Almoradi",
      "userId": "17505604726212925079"
     },
     "user_tz": -180
    },
    "id": "vXqr8bGbFhte",
    "outputId": "9763687f-6246-4444-96e3-d4c295345b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.16.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
      "google-adk 1.16.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install \"openai>=1.40.0\" \"chromadb>=0.5.4\" gradio pypdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1605,
     "status": "ok",
     "timestamp": 1760736156144,
     "user": {
      "displayName": "leena Almoradi",
      "userId": "17505604726212925079"
     },
     "user_tz": -180
    },
    "id": "AGFyxoJKGAIS"
   },
   "outputs": [],
   "source": [
    "import os, json, uuid, traceback\n",
    "from typing import List, Dict, Any\n",
    "from string import Template\n",
    "\n",
    "import gradio as gr\n",
    "from pypdf import PdfReader\n",
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "from openai import OpenAI\n",
    "\n",
    "from google.colab import userdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2217,
     "status": "ok",
     "timestamp": 1760736164448,
     "user": {
      "displayName": "leena Almoradi",
      "userId": "17505604726212925079"
     },
     "user_tz": -180
    },
    "id": "dgbsU25bGEN_"
   },
   "outputs": [],
   "source": [
    "# @title Config + API key\n",
    "COLLECTION_NAME = \"cvs_simple_kb\"\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "CHAT_MODEL  = \"gpt-4o-mini\"\n",
    "CHROMA_DIR  = \"/content/chroma_store\"   # change to a Drive path if you mount Drive\n",
    "\n",
    "# Ask for API key if missing\n",
    "client = OpenAI(api_key= userdata.get('OPENAI_API_KEY'))\n",
    "chroma_client: PersistentClient = chromadb.PersistentClient(path=CHROMA_DIR)\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1760736167103,
     "user": {
      "displayName": "leena Almoradi",
      "userId": "17505604726212925079"
     },
     "user_tz": -180
    },
    "id": "Tdb6IzJAGS39"
   },
   "outputs": [],
   "source": [
    "# @title Helpers\n",
    "\n",
    "def read_pdf_text(path: str) -> str:\n",
    "    txt = []\n",
    "    reader = PdfReader(path)\n",
    "    for p in reader.pages:\n",
    "        txt.append(p.extract_text() or \"\")\n",
    "    return \"\\n\".join(txt)\n",
    "\n",
    "def read_any_text(path: str) -> str:\n",
    "    path_l = path.lower()\n",
    "    if path_l.endswith(\".pdf\"):\n",
    "        return read_pdf_text(path)\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "    out = []\n",
    "    for i in range(0, len(texts), 64):\n",
    "        batch = texts[i:i+64]\n",
    "        resp = client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
    "        out.extend(d.embedding for d in resp.data)\n",
    "    return out\n",
    "\n",
    "def _normalize_file_list(files):\n",
    "    \"\"\"Gradio File(file_count='multiple', type='filepath') may return list of paths or file objs.\"\"\"\n",
    "    if not files:\n",
    "        return []\n",
    "    if isinstance(files, (str, os.PathLike)):\n",
    "        return [str(files)]\n",
    "    paths = []\n",
    "    for f in files:\n",
    "        p = getattr(f, \"name\", None) or (f if isinstance(f, str) else None)\n",
    "        if p:\n",
    "            paths.append(p)\n",
    "    return paths\n",
    "\n",
    "def _md_ok(msg):   return f\"✅ **{msg}**\"\n",
    "def _md_warn(msg): return f\"⚠️ **{msg}**\"\n",
    "def _md_err(msg):  return f\"❌ **{msg}**\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1760736170854,
     "user": {
      "displayName": "leena Almoradi",
      "userId": "17505604726212925079"
     },
     "user_tz": -180
    },
    "id": "i_KZ3JB2GVN7"
   },
   "outputs": [],
   "source": [
    "# @title LLM: chunk + metadata\n",
    "\n",
    "# Use $cv_text placeholder so JSON braces don't interfere\n",
    "CHUNK_META_PROMPT = Template(\"\"\"You are helping HR index a candidate CV.\n",
    "Return STRICT JSON with this schema:\n",
    "{\n",
    "  \"candidate\": {\n",
    "    \"name\": string|null,\n",
    "    \"email\": string|null,\n",
    "    \"phone\": string|null,\n",
    "    \"location\": string|null,\n",
    "    \"years_experience\": number|null,\n",
    "    \"seniority\": \"Junior\"|\"Mid\"|\"Senior\"|null,\n",
    "    \"role\": string|null,\n",
    "    \"skills\": string[]\n",
    "  },\n",
    "  \"chunks\": [\n",
    "    {\n",
    "      \"text\": string,\n",
    "      \"section\": string|null\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Split into 4–20 coherent chunks (by section/paragraph). Each chunk <= 800 chars.\n",
    "- Do not invent facts; if unknown, use null or [].\n",
    "- skills should be concise lowercase tokens.\n",
    "- Output ONLY JSON.\n",
    "\n",
    "CV TEXT:\n",
    "\\\"\\\"\\\"$cv_text\\\"\\\"\\\"\"\"\")\n",
    "\n",
    "def llm_chunk_and_metadata(cv_text: str) -> Dict[str, Any]:\n",
    "    # keep prompt length reasonable\n",
    "    msg = CHUNK_META_PROMPT.substitute(cv_text=cv_text[:12000])\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[{\"role\":\"user\",\"content\": msg}],\n",
    "        temperature=0\n",
    "    )\n",
    "    raw = resp.choices[0].message.content\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except Exception:\n",
    "        # Fallback minimal structure if LLM returns invalid JSON\n",
    "        data = {\n",
    "            \"candidate\": {\n",
    "                \"name\": None, \"email\": None, \"phone\": None, \"location\": None,\n",
    "                \"years_experience\": None, \"seniority\": None, \"role\": None, \"skills\": []\n",
    "            },\n",
    "            \"chunks\": [{\"text\": cv_text[:800], \"section\": \"full_text\"}]\n",
    "        }\n",
    "    # sanitize\n",
    "    chunks = [c for c in data.get(\"chunks\", []) if c and c.get(\"text\")]\n",
    "    data[\"chunks\"] = chunks\n",
    "    if \"candidate\" not in data or not isinstance(data[\"candidate\"], dict):\n",
    "        data[\"candidate\"] = {\n",
    "            \"name\": None, \"email\": None, \"phone\": None, \"location\": None,\n",
    "            \"years_experience\": None, \"seniority\": None, \"role\": None, \"skills\": []\n",
    "        }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1760736175610,
     "user": {
      "displayName": "leena Almoradi",
      "userId": "17505604726212925079"
     },
     "user_tz": -180
    },
    "id": "2I1CuNmEGXls"
   },
   "outputs": [],
   "source": [
    "import json, os, uuid, traceback\n",
    "\n",
    "def _coerce_metadata(md: dict) -> dict:\n",
    "    \"\"\"Chroma 0.5 metadata must be scalar/None. Convert lists/dicts safely.\"\"\"\n",
    "    out = {}\n",
    "    for k, v in md.items():\n",
    "        if isinstance(v, list):\n",
    "            out[k] = \", \".join(str(x) for x in v) if v else None\n",
    "        elif isinstance(v, (dict, set, tuple)):\n",
    "            out[k] = json.dumps(list(v) if not isinstance(v, dict) else v, ensure_ascii=False)\n",
    "        elif isinstance(v, (str, int, float, bool)) or v is None:\n",
    "            out[k] = v\n",
    "        else:\n",
    "            out[k] = str(v)\n",
    "    return out\n",
    "\n",
    "def index_cvs(files, default_tags: str = \"cv\") -> str:\n",
    "    try:\n",
    "        file_paths = _normalize_file_list(files)\n",
    "        if not file_paths:\n",
    "            return _md_warn(\"No files received. Ensure File component is `type=\\\"filepath\\\"` and files are selected.\")\n",
    "\n",
    "        tags_input = [t.strip() for t in (default_tags or \"\").split(\",\") if t.strip()]\n",
    "        total_chunks, lines = 0, [f\"### Index report ({len(file_paths)} file(s))\"]\n",
    "\n",
    "        for path in file_paths:\n",
    "            try:\n",
    "                file_name = os.path.basename(path)\n",
    "                text = read_any_text(path)\n",
    "                if not text.strip():\n",
    "                    lines.append(_md_warn(f\"{file_name}: extracted empty text (scanned PDF without OCR?). Skipping.\"))\n",
    "                    continue\n",
    "\n",
    "                parsed = llm_chunk_and_metadata(text)\n",
    "                cand   = parsed.get(\"candidate\", {}) or {}\n",
    "                chunks = [c for c in parsed.get(\"chunks\", []) if c.get(\"text\")]\n",
    "                if not chunks:\n",
    "                    lines.append(_md_warn(f\"{file_name}: LLM returned 0 chunks. Skipping.\"))\n",
    "                    continue\n",
    "\n",
    "                candidate_id = str(uuid.uuid4())\n",
    "                docs = [c[\"text\"][:2000] for c in chunks]\n",
    "                embs = embed_texts(docs)\n",
    "                ids  = [f\"{candidate_id}-c{i}\" for i in range(len(docs))]\n",
    "\n",
    "                # Build raw metadata, then coerce types for Chroma\n",
    "                raw_mds = [{\n",
    "                    \"candidate_id\": candidate_id,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"section\": c.get(\"section\"),\n",
    "                    \"name\": cand.get(\"name\"),\n",
    "                    \"email\": cand.get(\"email\"),\n",
    "                    \"phone\": cand.get(\"phone\"),\n",
    "                    \"location\": cand.get(\"location\"),\n",
    "                    \"years_experience\": cand.get(\"years_experience\"),\n",
    "                    \"seniority\": cand.get(\"seniority\"),\n",
    "                    \"role\": cand.get(\"role\"),\n",
    "                    \"skills\": cand.get(\"skills\") or [],   # list → will be coerced\n",
    "                    \"tags\": tags_input,                   # list → will be coerced\n",
    "                    \"doc_type\": \"cv\",\n",
    "                } for c in chunks]\n",
    "\n",
    "                metadatas = [_coerce_metadata(md) for md in raw_mds]\n",
    "\n",
    "                collection.add(ids=ids, documents=docs, metadatas=metadatas, embeddings=embs)\n",
    "                total_chunks += len(docs)\n",
    "                lines.append(_md_ok(f\"{file_name}: indexed {len(docs)} chunk(s). Candidate: {cand.get('name') or 'Unknown'}\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                lines.append(_md_err(f\"{os.path.basename(path)}: {e}\"))\n",
    "                lines.append(\"```text\\n\" + traceback.format_exc() + \"\\n```\")\n",
    "\n",
    "        if total_chunks == 0:\n",
    "            lines.append(_md_warn(\"No chunks were added. Check errors above (API key, PDF text, etc.).\"))\n",
    "        return \"\\n\\n\".join(lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        return _md_err(f\"Top-level error: {e}\") + \"\\n\\n```text\\n\" + traceback.format_exc() + \"\\n```\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1760736179808,
     "user": {
      "displayName": "leena Almoradi",
      "userId": "17505604726212925079"
     },
     "user_tz": -180
    },
    "id": "Aa0pkz47GaN5"
   },
   "outputs": [],
   "source": [
    "# @title Search (JD match)\n",
    "\n",
    "def search_candidates(jd_text: str,\n",
    "                      top_k_candidates: int = 10,\n",
    "                      seniority: str = \"\", location: str = \"\", role_hint: str = \"\") -> List[Dict[str, Any]]:\n",
    "    where = {\"doc_type\":\"cv\"}\n",
    "    if seniority.strip(): where[\"seniority\"] = seniority.strip().title()\n",
    "    if location.strip():  where[\"location\"]  = location.strip()\n",
    "    if role_hint.strip(): where[\"role\"]      = role_hint.strip().title()\n",
    "\n",
    "    qvec = embed_texts([jd_text or \"\"])[0]\n",
    "    res = collection.query(\n",
    "        query_embeddings=[qvec],\n",
    "        n_results=200,\n",
    "        where=where,\n",
    "        include=[\"documents\",\"metadatas\",\"distances\"]\n",
    "    )\n",
    "    docs = res.get(\"documents\",[[]])[0]\n",
    "    metas = res.get(\"metadatas\",[[]])[0]\n",
    "    dists = res.get(\"distances\",[[]])[0]\n",
    "\n",
    "    by_cand: Dict[str, Dict[str, Any]] = {}\n",
    "    for doc, md, dist in zip(docs, metas, dists):\n",
    "        cid = md.get(\"candidate_id\", \"unknown\")\n",
    "        if cid not in by_cand:\n",
    "            by_cand[cid] = {\n",
    "                \"Score\": 999.0,\n",
    "                \"Name\": md.get(\"name\"), \"Email\": md.get(\"email\"), \"Phone\": md.get(\"phone\"),\n",
    "                \"Location\": md.get(\"location\"), \"YearsExp\": md.get(\"years_experience\"),\n",
    "                \"Seniority\": md.get(\"seniority\"), \"Role\": md.get(\"role\"),\n",
    "                \"Skills\": \", \".join(md.get(\"skills\") or []),\n",
    "                \"Snippets\": []\n",
    "            }\n",
    "        d = float(dist)\n",
    "        if d < by_cand[cid][\"Score\"]:\n",
    "            by_cand[cid][\"Score\"] = d\n",
    "        if len(by_cand[cid][\"Snippets\"]) < 2:\n",
    "            by_cand[cid][\"Snippets\"].append(doc[:220] + (\"...\" if len(doc) > 220 else \"\"))\n",
    "\n",
    "    rows = list(by_cand.values())\n",
    "    rows.sort(key=lambda r: r[\"Score\"])           # lower distance first\n",
    "    for r in rows:\n",
    "        r[\"Score\"] = round(1.0/(1.0 + r[\"Score\"]), 4)   # convert to similarity-ish\n",
    "    return rows[:top_k_candidates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1760736182735,
     "user": {
      "displayName": "leena Almoradi",
      "userId": "17505604726212925079"
     },
     "user_tz": -180
    },
    "id": "C7vBCCV5Gg-j"
   },
   "outputs": [],
   "source": [
    "# --- Q&A: dedupe by candidate and flatten output ---\n",
    "\n",
    "RAG_SYSTEM = (\n",
    "    \"You are an HR assistant. Answer ONLY from the provided CONTEXT.\\n\"\n",
    "    \"If the answer is not present, say you don't know. Be concise.\"\n",
    ")\n",
    "\n",
    "def qa_over_cvs_dedup(question: str, top_k_chunks: int = 6,\n",
    "                      seniority: str = \"\", location: str = \"\", role_hint: str = \"\", keyword: str = \"\"):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\", []\n",
    "\n",
    "    # 1) Retrieve (overfetch) then filter\n",
    "    qvec = embed_texts([question])[0].reshape(1, -1)\n",
    "    scores, rows = faiss_store.search(qvec, top_k=top_k_chunks * 6)\n",
    "    scores, rows = scores[0], rows[0]\n",
    "\n",
    "    # 2) Build context (keep up to top_k_chunks unique chunks after filters)\n",
    "    ctx_parts = []\n",
    "    per_candidate = {}  # candidate_id -> {score, name, role, seniority, location, snippet}\n",
    "    kept = 0\n",
    "    for s, r in zip(scores, rows):\n",
    "        if r == -1:\n",
    "            continue\n",
    "        _id = faiss_store.row2id[r]\n",
    "        rec = faiss_store.store[_id]\n",
    "        md  = rec[\"metadata\"]\n",
    "        doc = rec[\"document\"]\n",
    "\n",
    "        # filters\n",
    "        if seniority and (md.get(\"seniority\") or \"\") != seniority:   continue\n",
    "        if location  and (md.get(\"location\")  or \"\") != location:    continue\n",
    "        if role_hint and (md.get(\"role\")      or \"\") != role_hint:   continue\n",
    "        if keyword   and (keyword.lower() not in (doc or \"\").lower()): continue\n",
    "\n",
    "        # add to LLM context until limit\n",
    "        if kept < top_k_chunks:\n",
    "            label = f\"{md.get('name') or md.get('file_name')} | {md.get('section')}\"\n",
    "            ctx_parts.append(f\"[{kept+1} | {label}]\\n{doc}\")\n",
    "            kept += 1\n",
    "\n",
    "        # aggregate per candidate (best score + first good snippet)\n",
    "        cid = md.get(\"candidate_id\") or _id\n",
    "        current = per_candidate.get(cid, {\n",
    "            \"candidate\": md.get(\"name\") or md.get(\"file_name\"),\n",
    "            \"role\": md.get(\"role\") or \"\",\n",
    "            \"seniority\": md.get(\"seniority\") or \"\",\n",
    "            \"location\": md.get(\"location\") or \"\",\n",
    "            \"score\": 0.0,\n",
    "            \"snippet\": \"\"\n",
    "        })\n",
    "        if float(s) > current[\"score\"]:\n",
    "            current[\"score\"] = float(s)\n",
    "            # keep a short snippet for the table\n",
    "            current[\"snippet\"] = (doc[:220] + (\"...\" if len(doc) > 220 else \"\"))\n",
    "        per_candidate[cid] = current\n",
    "\n",
    "    if not ctx_parts:\n",
    "        return \"No relevant information found.\", []\n",
    "\n",
    "    # 3) Ask LLM with grounded context\n",
    "    ctx = \"\\n\\n\".join(ctx_parts)\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\": RAG_SYSTEM},\n",
    "        {\"role\":\"user\",\"content\": f\"QUESTION: {question}\\n\\nCONTEXT:\\n{ctx}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=CHAT_MODEL, messages=messages, temperature=0)\n",
    "    answer = resp.choices[0].message.content\n",
    "\n",
    "    # 4) Build a clean, flat table (no objects)\n",
    "    rows = []\n",
    "    for i, (_, c) in enumerate(sorted(per_candidate.items(), key=lambda kv: kv[1][\"score\"], reverse=True), start=1):\n",
    "        rows.append([\n",
    "            i,\n",
    "            c[\"candidate\"],\n",
    "            c[\"role\"],\n",
    "            c[\"seniority\"],\n",
    "            c[\"location\"],\n",
    "            round(c[\"score\"], 4),\n",
    "            c[\"snippet\"]\n",
    "        ])\n",
    "    return answer, rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "06XdGsAkGjPR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://582ebe95d5879d8506.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://582ebe95d5879d8506.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title UI\n",
    "with gr.Blocks(title=\"Simple HR CV Search — LLM Chunk + Metadata\") as app:\n",
    "    gr.Markdown(\"### Simple HR RAG: Upload CVs → LLM chunks & metadata → Search/JD → Q&A\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Ingest\"):\n",
    "            up = gr.File(file_count=\"multiple\", type=\"filepath\", label=\"Upload CVs (PDF/TXT)\")\n",
    "            tags = gr.Textbox(value=\"cv\", label=\"Default tags (optional)\")\n",
    "            go = gr.Button(\"Index\", variant=\"primary\")\n",
    "            out = gr.Markdown()\n",
    "            go.click(index_cvs, inputs=[up, tags], outputs=out)\n",
    "\n",
    "        with gr.TabItem(\"Search (JD match)\"):\n",
    "            jd   = gr.Textbox(lines=6, label=\"Job Description\")\n",
    "            with gr.Row():\n",
    "                sen = gr.Dropdown(choices=[\"\",\"Junior\",\"Mid\",\"Senior\"], value=\"\", label=\"Seniority\")\n",
    "                loc = gr.Textbox(label=\"Location\")\n",
    "                role = gr.Textbox(label=\"Role\")\n",
    "            topk = gr.Slider(1, 30, value=10, step=1, label=\"Top-K candidates\")\n",
    "            btn  = gr.Button(\"Search\", variant=\"primary\")\n",
    "            tbl  = gr.Dataframe(\n",
    "                wrap=True,\n",
    "                headers=[\"Score\",\"Name\",\"Email\",\"Phone\",\"Location\",\"YearsExp\",\"Seniority\",\"Role\",\"Skills\",\"Snippets\"],\n",
    "                interactive=False\n",
    "            )\n",
    "            btn.click(search_candidates, inputs=[jd, topk, sen, loc, role], outputs=tbl)\n",
    "\n",
    "        with gr.TabItem(\"Q&A\"):\n",
    "            q = gr.Textbox(lines=2, label=\"Question\", placeholder=\"e.g., Who has hands-on Airflow experience?\")\n",
    "            with gr.Row():\n",
    "                topk_c = gr.Slider(1, 12, value=6, step=1, label=\"Top-K chunks\")\n",
    "                sen2 = gr.Dropdown(choices=[\"\",\"Junior\",\"Mid\",\"Senior\"], value=\"\", label=\"Seniority\")\n",
    "                loc2 = gr.Textbox(label=\"Location\")\n",
    "                role2 = gr.Textbox(label=\"Role\")\n",
    "            kw = gr.Textbox(label=\"Keyword prefilter (optional)\", placeholder=\"e.g. Airflow\")\n",
    "            ask = gr.Button(\"Ask\", variant=\"primary\")\n",
    "            ans = gr.Markdown()\n",
    "            cites = gr.Dataframe(headers=[\"#\",\"candidate\",\"role\",\"seniority\",\"location\",\"distance\"], wrap=True, interactive=False)\n",
    "            ask.click(qa_over_cvs_dedup, inputs=[q, topk_c, sen2, loc2, role2, kw], outputs=[ans, cites])\n",
    "\n",
    "app.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "executionInfo": {
     "elapsed": 265904,
     "status": "ok",
     "timestamp": 1758676389498,
     "user": {
      "displayName": "Karim Nabil",
      "userId": "06457032885623724391"
     },
     "user_tz": -180
    },
    "id": "pMIlQklqKhPX",
    "outputId": "c7773c7b-0798-4837-d130-596ba9558c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://6263c0a9173d8e40ec.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6263c0a9173d8e40ec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7861 <> https://6263c0a9173d8e40ec.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1) Install & Imports\n",
    "# ===============================\n",
    "!pip -q install \"openai>=1.40.0\" gradio pypdf python-dotenv faiss-cpu\n",
    "\n",
    "import os, json, uuid, traceback, math, pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from string import Template\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import gradio as gr\n",
    "from pypdf import PdfReader\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "# ===============================\n",
    "# 2) Config & Init\n",
    "# ===============================\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "CHAT_MODEL  = \"gpt-4o-mini\"\n",
    "\n",
    "# Where we persist the FAISS index & store\n",
    "STORE_DIR   = \"/content/faiss_store\"\n",
    "INDEX_PATH  = os.path.join(STORE_DIR, \"index.faiss\")\n",
    "MAP_PATH    = os.path.join(STORE_DIR, \"store.json\")   # our sidecar: id -> {doc, metadata}\n",
    "DIM         = 1536                                    # text-embedding-3-small = 1536 dims\n",
    "\n",
    "os.makedirs(STORE_DIR, exist_ok=True)\n",
    "\n",
    "client = OpenAI(api_key= userdata.get('OPENAI_API_KEY'))\n",
    "\n",
    "# ===============================\n",
    "# 3) Helpers (IO, embeddings, UI)\n",
    "# ===============================\n",
    "def read_pdf_text(path: str) -> str:\n",
    "    txt = []\n",
    "    reader = PdfReader(path)\n",
    "    for p in reader.pages:\n",
    "        txt.append(p.extract_text() or \"\")\n",
    "    return \"\\n\".join(txt)\n",
    "\n",
    "def read_any_text(path: str) -> str:\n",
    "    return read_pdf_text(path) if path.lower().endswith(\".pdf\") else open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Return L2-normalized embeddings (needed for cosine via inner product).\"\"\"\n",
    "    out = []\n",
    "    for i in range(0, len(texts), 64):\n",
    "        batch = texts[i:i+64]\n",
    "        resp = client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
    "        out.extend(d.embedding for d in resp.data)\n",
    "    arr = np.array(out, dtype=\"float32\")\n",
    "    # normalize to unit length for cosine via IP\n",
    "    norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12\n",
    "    arr = arr / norms\n",
    "    return arr\n",
    "\n",
    "def _normalize_file_list(files):\n",
    "    if not files: return []\n",
    "    if isinstance(files, (str, os.PathLike)): return [str(files)]\n",
    "    paths = []\n",
    "    for f in files:\n",
    "        p = getattr(f, \"name\", None) or (f if isinstance(f, str) else None)\n",
    "        if p: paths.append(p)\n",
    "    return paths\n",
    "\n",
    "def _md_ok(msg):   return f\"✅ **{msg}**\"\n",
    "def _md_warn(msg): return f\"⚠️ **{msg}**\"\n",
    "def _md_err(msg):  return f\"❌ **{msg}**\"\n",
    "# ---- Put near your other helpers ----\n",
    "TABLE_HEADERS = [\"Score\",\"Name\",\"Email\",\"Phone\",\"Location\",\"YearsExp\",\"Seniority\",\"Role\",\"Skills\",\"Snippets\"]\n",
    "\n",
    "def _rows_to_table(rows):\n",
    "    \"\"\"Flatten list-of-dicts into list-of-lists with only scalar values.\"\"\"\n",
    "    table = []\n",
    "    for r in rows or []:\n",
    "        table.append([\n",
    "            r.get(\"Score\", 0),\n",
    "            r.get(\"Name\") or \"\",\n",
    "            r.get(\"Email\") or \"\",\n",
    "            r.get(\"Phone\") or \"\",\n",
    "            r.get(\"Location\") or \"\",\n",
    "            r.get(\"YearsExp\") if r.get(\"YearsExp\") is not None else \"\",\n",
    "            r.get(\"Seniority\") or \"\",\n",
    "            r.get(\"Role\") or \"\",\n",
    "            r.get(\"Skills\") or \"\",\n",
    "            \" | \".join(r.get(\"Snippets\") or []),  # stringify list\n",
    "        ])\n",
    "    return table\n",
    "def do_search_for_table(jd, topk, sen, loc, role):\n",
    "    rows = search_candidates(jd_text=jd,\n",
    "                             top_k_candidates=topk,\n",
    "                             seniority=sen or \"\",\n",
    "                             location=loc or \"\",\n",
    "                             role_hint=role or \"\")\n",
    "    return _rows_to_table(rows)\n",
    "\n",
    "\n",
    "# ---- Q&A table helpers ----\n",
    "CITES_HEADERS = [\"#\", \"Candidate\", \"Role\", \"Seniority\", \"Location\", \"Score\"]\n",
    "\n",
    "def _cites_to_table(cites):\n",
    "    table = []\n",
    "    for c in cites or []:\n",
    "        # ensure only scalars go into the dataframe\n",
    "        score = c.get(\"score\", \"\")\n",
    "        try:\n",
    "            score = round(float(score), 4)\n",
    "        except Exception:\n",
    "            pass\n",
    "        table.append([\n",
    "            c.get(\"#\", \"\"),\n",
    "            c.get(\"candidate\") or \"\",\n",
    "            c.get(\"role\") or \"\",\n",
    "            c.get(\"seniority\") or \"\",\n",
    "            c.get(\"location\") or \"\",\n",
    "            score,\n",
    "        ])\n",
    "    return table\n",
    "\n",
    "# ===============================\n",
    "# 4) FAISS Index Manager\n",
    "# ===============================\n",
    "class FaissStore:\n",
    "    \"\"\"\n",
    "    - FAISS index (IndexFlatIP) over L2-normalized vectors  => cosine similarity\n",
    "    - Python dict sidecar: id -> {\"document\": str, \"metadata\": {...}}\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, index_path: str, map_path: str):\n",
    "        self.dim = dim\n",
    "        self.index_path = index_path\n",
    "        self.map_path = map_path\n",
    "\n",
    "        self.id2row: Dict[str, int] = {}      # id -> row number in FAISS\n",
    "        self.row2id: List[str] = []           # row -> id\n",
    "        self.store: Dict[str, Dict[str, Any]] = {}  # id -> payload\n",
    "\n",
    "        if os.path.exists(index_path) and os.path.exists(map_path):\n",
    "            self._load()\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatIP(dim)\n",
    "\n",
    "    def add(self, ids: List[str], vectors: np.ndarray, documents: List[str], metadatas: List[Dict[str, Any]]):\n",
    "        assert vectors.shape[0] == len(ids) == len(documents) == len(metadatas)\n",
    "        # Add to FAISS\n",
    "        self.index.add(vectors)\n",
    "        # Update mappings\n",
    "        start_row = len(self.row2id)\n",
    "        for i, _id in enumerate(ids):\n",
    "            row = start_row + i\n",
    "            self.row2id.append(_id)\n",
    "            self.id2row[_id] = row\n",
    "            self.store[_id] = {\"document\": documents[i], \"metadata\": metadatas[i]}\n",
    "\n",
    "    def search(self, query_vectors: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Return (scores, row_indices). Scores are inner product (cosine).\"\"\"\n",
    "        scores, idxs = self.index.search(query_vectors, top_k)\n",
    "        return scores, idxs\n",
    "\n",
    "    def save(self):\n",
    "        faiss.write_index(self.index, self.index_path)\n",
    "        with open(self.map_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"row2id\": self.row2id,\n",
    "                \"store\": self.store\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    def _load(self):\n",
    "        self.index = faiss.read_index(self.index_path)\n",
    "        with open(self.map_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        self.row2id = data.get(\"row2id\", [])\n",
    "        self.store  = data.get(\"store\", {})\n",
    "        self.id2row = {i: r for r, i in enumerate(self.row2id)}\n",
    "\n",
    "def do_qa_for_table(q, topk, sen, loc, role, kw):\n",
    "    answer, cites = qa_over_cvs(\n",
    "        question=q,\n",
    "        top_k_chunks=topk,\n",
    "        seniority=sen or \"\",\n",
    "        location=loc or \"\",\n",
    "        role_hint=role or \"\",\n",
    "        keyword=kw or \"\",\n",
    "    )\n",
    "    return answer, _cites_to_table(cites)\n",
    "\n",
    "\n",
    "# global store\n",
    "faiss_store = FaissStore(DIM, INDEX_PATH, MAP_PATH)\n",
    "\n",
    "# ===============================\n",
    "# 5) LLM: chunk + metadata (simple)\n",
    "# ===============================\n",
    "CHUNK_META_PROMPT = Template(\"\"\"You are helping HR index a candidate CV.\n",
    "Return STRICT JSON with this schema:\n",
    "{\n",
    "  \"candidate\": {\n",
    "    \"name\": string|null,\n",
    "    \"email\": string|null,\n",
    "    \"phone\": string|null,\n",
    "    \"location\": string|null,\n",
    "    \"years_experience\": number|null,\n",
    "    \"seniority\": \"Junior\"|\"Mid\"|\"Senior\"|null,\n",
    "    \"role\": string|null,\n",
    "    \"skills\": string[]\n",
    "  },\n",
    "  \"chunks\": [\n",
    "    {\n",
    "      \"text\": string,\n",
    "      \"section\": string|null\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Split into 4–20 coherent chunks; each chunk <= 800 characters.\n",
    "- Do not invent facts; if unknown, use null or [].\n",
    "- skills must be short lowercase tokens.\n",
    "- Output ONLY JSON.\n",
    "\n",
    "CV TEXT:\n",
    "\\\"\\\"\\\"$cv_text\\\"\\\"\\\"\"\"\")\n",
    "\n",
    "import re, json\n",
    "from string import Template\n",
    "\n",
    "# keep the same CHUNK_META_PROMPT you already have\n",
    "# CHUNK_META_PROMPT = Template(\"\"\" ... $cv_text ... \"\"\")\n",
    "\n",
    "NAME_RE  = re.compile(r\"(?im)^\\s*([A-Z][A-Za-z]+(?:\\s+[A-Z][A-Za-z'’\\-]+){0,3})\\s*$\")\n",
    "EMAIL_RE = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "PHONE_RE = re.compile(r\"(\\+?\\d[\\d\\-\\s\\(\\)]{7,})\")\n",
    "\n",
    "def _fallback_profile_from_text(t: str) -> dict:\n",
    "    lines = [ln.strip() for ln in (t or \"\").splitlines() if ln.strip()]\n",
    "    name = None\n",
    "    for ln in lines[:8]:\n",
    "        m = NAME_RE.match(ln)\n",
    "        if m:\n",
    "            name = m.group(1)\n",
    "            break\n",
    "    email = (EMAIL_RE.search(t) or [None])[0] if EMAIL_RE.search(t) else None\n",
    "    phone = (PHONE_RE.search(t) or [None])[0] if PHONE_RE.search(t) else None\n",
    "    return {\n",
    "        \"name\": name, \"email\": email, \"phone\": phone,\n",
    "        \"location\": None, \"years_experience\": None,\n",
    "        \"seniority\": None, \"role\": None, \"skills\": []\n",
    "    }\n",
    "\n",
    "def llm_chunk_and_metadata(cv_text: str) -> dict:\n",
    "    \"\"\"Stricter JSON mode + single retry; if still invalid, fall back to heuristics and multi-split.\"\"\"\n",
    "    prompt = CHUNK_META_PROMPT.substitute(cv_text=cv_text[:12000])\n",
    "    def _ask():\n",
    "        return client.chat.completions.create(\n",
    "            model=CHAT_MODEL,\n",
    "            messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "            temperature=0,\n",
    "            response_format={ \"type\": \"json_object\" }  # <-- JSON mode (supported by gpt-4o-mini)\n",
    "        )\n",
    "\n",
    "    raw = None\n",
    "    for _ in range(2):  # try twice\n",
    "        resp = _ask()\n",
    "        raw = resp.choices[0].message.content\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "            # minimal sanity\n",
    "            if isinstance(data.get(\"chunks\"), list) and len(data[\"chunks\"]) >= 2:\n",
    "                return data\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # If we are here, LLM gave invalid JSON or too few chunks → heuristic fallback\n",
    "    profile = _fallback_profile_from_text(cv_text)\n",
    "    # simple multi-split fallback so you don't end up with a single chunk\n",
    "    text = \"\\n\".join(ln.strip() for ln in cv_text.splitlines())\n",
    "    chunks, size, overlap = [], 800, 120\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        j = min(len(text), i + size)\n",
    "        window = text[i:j]\n",
    "        last = max(window.rfind(\"\\n\\n\"), window.rfind(\". \"))\n",
    "        if last != -1 and (j - i) > 200:\n",
    "            j = i + last + (2 if window[last:last+2] == \"\\n\\n\" else 1)\n",
    "        chunk = text[i:j].strip()\n",
    "        if chunk:\n",
    "            chunks.append({\"text\": chunk, \"section\": None})\n",
    "        if j >= len(text): break\n",
    "        i = max(j - overlap, 0)\n",
    "        if i == j: break\n",
    "\n",
    "    return {\n",
    "        \"candidate\": profile,\n",
    "        \"chunks\": chunks if chunks else [{\"text\": cv_text[:800], \"section\": \"full_text\"}],\n",
    "    }\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6) Ingest (multi-file)\n",
    "# ===============================\n",
    "def index_cvs(files, default_tags: str = \"cv\") -> str:\n",
    "    try:\n",
    "        file_paths = _normalize_file_list(files)\n",
    "        if not file_paths:\n",
    "            return _md_warn(\"No files selected.\")\n",
    "\n",
    "        tags = [t.strip() for t in (default_tags or \"\").split(\",\") if t.strip()]\n",
    "        total_chunks = 0\n",
    "        lines = [f\"### Index report ({len(file_paths)} file(s))\"]\n",
    "\n",
    "        for path in file_paths:\n",
    "            try:\n",
    "                file_name = os.path.basename(path)\n",
    "                text = read_any_text(path)\n",
    "                if not text.strip():\n",
    "                    lines.append(_md_warn(f\"{file_name}: empty text (scanned PDF?). Skipping.\"))\n",
    "                    continue\n",
    "\n",
    "                parsed = llm_chunk_and_metadata(text)\n",
    "                cand   = parsed.get(\"candidate\", {}) or {}\n",
    "                chunks = [c for c in parsed.get(\"chunks\", []) if c.get(\"text\")]\n",
    "                if not chunks:\n",
    "                    lines.append(_md_warn(f\"{file_name}: LLM returned 0 chunks. Skipping.\"))\n",
    "                    continue\n",
    "\n",
    "                candidate_id = str(uuid.uuid4())\n",
    "                docs   = [c[\"text\"][:2000] for c in chunks]  # safety cap\n",
    "                embs   = embed_texts(docs)\n",
    "                ids    = [f\"{candidate_id}-c{i}\" for i in range(len(docs))]\n",
    "                metas  = [{\n",
    "                    \"candidate_id\": candidate_id,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"section\": c.get(\"section\"),\n",
    "                    \"name\": cand.get(\"name\"),\n",
    "                    \"email\": cand.get(\"email\"),\n",
    "                    \"phone\": cand.get(\"phone\"),\n",
    "                    \"location\": cand.get(\"location\"),\n",
    "                    \"years_experience\": cand.get(\"years_experience\"),\n",
    "                    \"seniority\": cand.get(\"seniority\"),\n",
    "                    \"role\": cand.get(\"role\"),\n",
    "                    \"skills\": \", \".join(cand.get(\"skills\") or []),   # store as string in sidecar\n",
    "                    \"tags\": \", \".join(tags),\n",
    "                    \"doc_type\": \"cv\",\n",
    "                } for c in chunks]\n",
    "\n",
    "                # Add to FAISS store (no type restriction headaches)\n",
    "                faiss_store.add(ids, embs, docs, metas)\n",
    "                total_chunks += len(docs)\n",
    "                lines.append(_md_ok(f\"{file_name}: indexed {len(docs)} chunk(s). Candidate: {cand.get('name') or 'Unknown'}\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                lines.append(_md_err(f\"{os.path.basename(path)}: {e}\"))\n",
    "                lines.append(\"```text\\n\" + traceback.format_exc() + \"\\n```\")\n",
    "\n",
    "        if total_chunks == 0:\n",
    "            lines.append(_md_warn(\"No chunks were added. Check errors above (API key, PDF text, etc.).\"))\n",
    "        else:\n",
    "            faiss_store.save()\n",
    "            lines.append(_md_ok(f\"Saved index with total chunks: {len(faiss_store.row2id)}\"))\n",
    "        return \"\\n\\n\".join(lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        return _md_err(f\"Top-level error: {e}\") + \"\\n\\n```text\\n\" + traceback.format_exc() + \"\\n```\"\n",
    "\n",
    "# ===============================\n",
    "# 7) Search (JD match) — group by candidate\n",
    "# ===============================\n",
    "def search_candidates(jd_text: str,\n",
    "                      top_k_candidates: int = 10,\n",
    "                      seniority: str = \"\", location: str = \"\", role_hint: str = \"\") -> List[Dict[str, Any]]:\n",
    "    if not jd_text.strip():\n",
    "        return []\n",
    "\n",
    "    qvec = embed_texts([jd_text])[0].reshape(1, -1)\n",
    "    scores, rows = faiss_store.search(qvec, top_k=200)   # pull wide\n",
    "    scores, rows = scores[0], rows[0]\n",
    "\n",
    "    # Gather hits, apply light metadata filters from sidecar\n",
    "    by_cand: Dict[str, Dict[str, Any]] = {}\n",
    "    for s, r in zip(scores, rows):\n",
    "        if r == -1: continue\n",
    "        _id = faiss_store.row2id[r]\n",
    "        rec = faiss_store.store[_id]\n",
    "        md  = rec[\"metadata\"]\n",
    "\n",
    "        if seniority and (md.get(\"seniority\") or \"\") != seniority:\n",
    "            continue\n",
    "        if location and (md.get(\"location\") or \"\") != location:\n",
    "            continue\n",
    "        if role_hint and (md.get(\"role\") or \"\") != role_hint:\n",
    "            continue\n",
    "\n",
    "        cid = md.get(\"candidate_id\", \"unknown\")\n",
    "        if cid not in by_cand:\n",
    "            by_cand[cid] = {\n",
    "                \"Score\": 0.0,\n",
    "                \"Name\": md.get(\"name\"),\n",
    "                \"Email\": md.get(\"email\"),\n",
    "                \"Phone\": md.get(\"phone\"),\n",
    "                \"Location\": md.get(\"location\"),\n",
    "                \"YearsExp\": md.get(\"years_experience\"),\n",
    "                \"Seniority\": md.get(\"seniority\"),\n",
    "                \"Role\": md.get(\"role\"),\n",
    "                \"Skills\": md.get(\"skills\"),\n",
    "                \"Snippets\": []\n",
    "            }\n",
    "        by_cand[cid][\"Score\"] = max(by_cand[cid][\"Score\"], float(s))  # use best (max) cosine\n",
    "        if len(by_cand[cid][\"Snippets\"]) < 2:\n",
    "            doc = rec[\"document\"]\n",
    "            by_cand[cid][\"Snippets\"].append(doc[:220] + (\"...\" if len(doc) > 220 else \"\"))\n",
    "\n",
    "    rows_out = list(by_cand.values())\n",
    "    rows_out.sort(key=lambda r: r[\"Score\"], reverse=True)\n",
    "    for r in rows_out:\n",
    "        r[\"Score\"] = round(r[\"Score\"], 4)\n",
    "    return rows_out[:top_k_candidates]\n",
    "\n",
    "# ===============================\n",
    "# 8) Q&A (grounded) with optional keyword prefilter\n",
    "# ===============================\n",
    "# --- Q&A: dedupe by candidate and flatten output ---\n",
    "\n",
    "RAG_SYSTEM = (\n",
    "    \"You are an HR assistant. Answer ONLY from the provided CONTEXT.\\n\"\n",
    "    \"If the answer is not present, say you don't know. Be concise.\"\n",
    ")\n",
    "\n",
    "def qa_over_cvs_dedup(question: str, top_k_chunks: int = 6,\n",
    "                      seniority: str = \"\", location: str = \"\", role_hint: str = \"\", keyword: str = \"\"):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\", []\n",
    "\n",
    "    # 1) Retrieve (overfetch) then filter\n",
    "    qvec = embed_texts([question])[0].reshape(1, -1)\n",
    "    scores, rows = faiss_store.search(qvec, top_k=top_k_chunks * 6)\n",
    "    scores, rows = scores[0], rows[0]\n",
    "\n",
    "    # 2) Build context (keep up to top_k_chunks unique chunks after filters)\n",
    "    ctx_parts = []\n",
    "    per_candidate = {}  # candidate_id -> {score, name, role, seniority, location, snippet}\n",
    "    kept = 0\n",
    "    for s, r in zip(scores, rows):\n",
    "        if r == -1:\n",
    "            continue\n",
    "        _id = faiss_store.row2id[r]\n",
    "        rec = faiss_store.store[_id]\n",
    "        md  = rec[\"metadata\"]\n",
    "        doc = rec[\"document\"]\n",
    "\n",
    "        # filters\n",
    "        if seniority and (md.get(\"seniority\") or \"\") != seniority:   continue\n",
    "        if location  and (md.get(\"location\")  or \"\") != location:    continue\n",
    "        if role_hint and (md.get(\"role\")      or \"\") != role_hint:   continue\n",
    "        if keyword   and (keyword.lower() not in (doc or \"\").lower()): continue\n",
    "\n",
    "        # add to LLM context until limit\n",
    "        if kept < top_k_chunks:\n",
    "            label = f\"{md.get('name') or md.get('file_name')} | {md.get('section')}\"\n",
    "            ctx_parts.append(f\"[{kept+1} | {label}]\\n{doc}\")\n",
    "            kept += 1\n",
    "\n",
    "        # aggregate per candidate (best score + first good snippet)\n",
    "        cid = md.get(\"candidate_id\") or _id\n",
    "        current = per_candidate.get(cid, {\n",
    "            \"candidate\": md.get(\"name\") or md.get(\"file_name\"),\n",
    "            \"role\": md.get(\"role\") or \"\",\n",
    "            \"seniority\": md.get(\"seniority\") or \"\",\n",
    "            \"location\": md.get(\"location\") or \"\",\n",
    "            \"score\": 0.0,\n",
    "            \"snippet\": \"\"\n",
    "        })\n",
    "        if float(s) > current[\"score\"]:\n",
    "            current[\"score\"] = float(s)\n",
    "            # keep a short snippet for the table\n",
    "            current[\"snippet\"] = (doc[:220] + (\"...\" if len(doc) > 220 else \"\"))\n",
    "        per_candidate[cid] = current\n",
    "\n",
    "    if not ctx_parts:\n",
    "        return \"No relevant information found.\", []\n",
    "\n",
    "    # 3) Ask LLM with grounded context\n",
    "    ctx = \"\\n\\n\".join(ctx_parts)\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\": RAG_SYSTEM},\n",
    "        {\"role\":\"user\",\"content\": f\"QUESTION: {question}\\n\\nCONTEXT:\\n{ctx}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=CHAT_MODEL, messages=messages, temperature=0)\n",
    "    answer = resp.choices[0].message.content\n",
    "\n",
    "    # 4) Build a clean, flat table (no objects)\n",
    "    rows = []\n",
    "    for i, (_, c) in enumerate(sorted(per_candidate.items(), key=lambda kv: kv[1][\"score\"], reverse=True), start=1):\n",
    "        rows.append([\n",
    "            i,\n",
    "            c[\"candidate\"],\n",
    "            c[\"role\"],\n",
    "            c[\"seniority\"],\n",
    "            c[\"location\"],\n",
    "            round(c[\"score\"], 4),\n",
    "            c[\"snippet\"]\n",
    "        ])\n",
    "    return answer, rows\n",
    "\n",
    "QA_HEADERS = [\"#\", \"Candidate\", \"Role\", \"Seniority\", \"Location\", \"Score\", \"Snippet\"]\n",
    "# ===============================\n",
    "# 9) Gradio UI\n",
    "# ===============================\n",
    "with gr.Blocks(title=\"HR CV Search (FAISS) — LLM Chunk + Metadata\") as app:\n",
    "    gr.Markdown(\"### HR CV RAG (FAISS): Upload CVs → LLM chunks & metadata → Search/JD → Q&A\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Ingest\"):\n",
    "            up = gr.File(file_count=\"multiple\", type=\"filepath\", label=\"Upload CVs (PDF/TXT)\")\n",
    "            tags = gr.Textbox(value=\"cv\", label=\"Default tags (optional)\")  # saved in sidecar only\n",
    "            go = gr.Button(\"Index\", variant=\"primary\")\n",
    "            out = gr.Markdown()\n",
    "            go.click(index_cvs, inputs=[up, tags], outputs=out)\n",
    "\n",
    "        with gr.TabItem(\"Search (JD match)\"):\n",
    "            jd   = gr.Textbox(lines=6, label=\"Job Description\")\n",
    "            with gr.Row():\n",
    "                sen = gr.Dropdown(choices=[\"\",\"Junior\",\"Mid\",\"Senior\"], value=\"\", label=\"Seniority\")\n",
    "                loc = gr.Textbox(label=\"Location\")\n",
    "                role = gr.Textbox(label=\"Role\")\n",
    "            topk = gr.Slider(1, 30, value=10, step=1, label=\"Top-K candidates\")\n",
    "            btn  = gr.Button(\"Search\", variant=\"primary\")\n",
    "\n",
    "            tbl  = gr.Dataframe(\n",
    "                headers=TABLE_HEADERS,\n",
    "                wrap=True,\n",
    "                interactive=False,\n",
    "                row_count=(0, \"dynamic\"),\n",
    "                col_count=(len(TABLE_HEADERS), \"fixed\"),\n",
    "            )\n",
    "\n",
    "        btn.click(do_search_for_table, inputs=[jd, topk, sen, loc, role], outputs=tbl)\n",
    "\n",
    "\n",
    "        # --- Replace only the Q&A tab in your UI block ---\n",
    "        with gr.TabItem(\"Q&A\"):\n",
    "            q = gr.Textbox(lines=2, label=\"Question\", placeholder=\"e.g., Who has hands-on RAG experience?\")\n",
    "            with gr.Row():\n",
    "                topk_c = gr.Slider(1, 12, value=6, step=1, label=\"Top-K chunks (for context)\")\n",
    "                sen2 = gr.Dropdown(choices=[\"\",\"Junior\",\"Mid\",\"Senior\"], value=\"\", label=\"Seniority\")\n",
    "                loc2 = gr.Textbox(label=\"Location\")\n",
    "                role2 = gr.Textbox(label=\"Role\")\n",
    "            kw = gr.Textbox(label=\"Keyword prefilter (optional)\", placeholder=\"e.g. Airflow\")\n",
    "\n",
    "            ask = gr.Button(\"Ask\", variant=\"primary\")\n",
    "            ans = gr.Markdown()\n",
    "            cites = gr.Dataframe(headers=QA_HEADERS, wrap=True, interactive=False,\n",
    "                              row_count=(0, \"dynamic\"), col_count=(len(QA_HEADERS), \"fixed\"))\n",
    "\n",
    "        ask.click(qa_over_cvs_dedup,\n",
    "                  inputs=[q, topk_c, sen2, loc2, role2, kw],\n",
    "                  outputs=[ans, cites])\n",
    "\n",
    "\n",
    "\n",
    "app.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTW40PvcTYrf"
   },
   "outputs": [],
   "source": [
    "!pip -q install \"openai>=1.40.0\" gradio pypdf faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-k1C6E0YB0S"
   },
   "outputs": [],
   "source": [
    "import os, json, uuid, traceback, math, pickle\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from string import Template\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import gradio as gr\n",
    "from pypdf import PdfReader\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq-UWdp_YRUG"
   },
   "outputs": [],
   "source": [
    "# 2) Config & Init\n",
    "# ===============================\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "CHAT_MODEL  = \"gpt-4o-mini\"\n",
    "\n",
    "# Where we persist the FAISS index & store\n",
    "STORE_DIR   = \"/content/faiss_store\"\n",
    "INDEX_PATH  = os.path.join(STORE_DIR, \"index.faiss\")\n",
    "MAP_PATH    = os.path.join(STORE_DIR, \"store.json\")   # our sidecar: id -> {doc, metadata}\n",
    "DIM         = 1536                                    # text-embedding-3-small = 1536 dims\n",
    "\n",
    "os.makedirs(STORE_DIR, exist_ok=True)\n",
    "\n",
    "client = OpenAI(api_key= userdata.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ecr3JqPWZB7p"
   },
   "outputs": [],
   "source": [
    "def read_pdf_text(path: str) -> str:\n",
    "    txt = []\n",
    "    reader = PdfReader(path)\n",
    "    for p in reader.pages:\n",
    "        txt.append(p.extract_text() or \"\")\n",
    "    return \"\\n\".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAPOhEUkZKj2"
   },
   "outputs": [],
   "source": [
    "def read_any_text(path: str) -> str:\n",
    "    return read_pdf_text(path) if path.lower().endswith(\".pdf\") else open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Return L2-normalized embeddings (needed for cosine via inner product).\"\"\"\n",
    "    out = []\n",
    "    for i in range(0, len(texts), 64):\n",
    "        batch = texts[i:i+64]\n",
    "        resp = client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
    "        out.extend(d.embedding for d in resp.data)\n",
    "    arr = np.array(out, dtype=\"float32\")\n",
    "    # normalize to unit length for cosine via IP\n",
    "    norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12\n",
    "    arr = arr / norms\n",
    "    return arr\n",
    "\n",
    "def _normalize_file_list(files):\n",
    "    if not files: return []\n",
    "    if isinstance(files, (str, os.PathLike)): return [str(files)]\n",
    "    paths = []\n",
    "    for f in files:\n",
    "        p = getattr(f, \"name\", None) or (f if isinstance(f, str) else None)\n",
    "        if p: paths.append(p)\n",
    "    return paths\n",
    "\n",
    "def _md_ok(msg):   return f\"✅ **{msg}**\"\n",
    "def _md_warn(msg): return f\"⚠️ **{msg}**\"\n",
    "def _md_err(msg):  return f\"❌ **{msg}**\"\n",
    "# ---- Put near your other helpers ----\n",
    "TABLE_HEADERS = [\"Score\",\"Name\",\"Email\",\"Phone\",\"Location\",\"YearsExp\",\"Seniority\",\"Role\",\"Skills\",\"Snippets\"]\n",
    "\n",
    "def _rows_to_table(rows):\n",
    "    \"\"\"Flatten list-of-dicts into list-of-lists with only scalar values.\"\"\"\n",
    "    table = []\n",
    "    for r in rows or []:\n",
    "        table.append([\n",
    "            r.get(\"Score\", 0),\n",
    "            r.get(\"Name\") or \"\",\n",
    "            r.get(\"Email\") or \"\",\n",
    "            r.get(\"Phone\") or \"\",\n",
    "            r.get(\"Location\") or \"\",\n",
    "            r.get(\"YearsExp\") if r.get(\"YearsExp\") is not None else \"\",\n",
    "            r.get(\"Seniority\") or \"\",\n",
    "            r.get(\"Role\") or \"\",\n",
    "            r.get(\"Skills\") or \"\",\n",
    "            \" | \".join(r.get(\"Snippets\") or []),  # stringify list\n",
    "        ])\n",
    "    return table\n",
    "def do_search_for_table(jd, topk, sen, loc, role):\n",
    "    rows = search_candidates(jd_text=jd,\n",
    "                             top_k_candidates=topk,\n",
    "                             seniority=sen or \"\",\n",
    "                             location=loc or \"\",\n",
    "                             role_hint=role or \"\")\n",
    "    return _rows_to_table(rows)\n",
    "\n",
    "\n",
    "# ---- Q&A table helpers ----\n",
    "CITES_HEADERS = [\"#\", \"Candidate\", \"Role\", \"Seniority\", \"Location\", \"Score\"]\n",
    "\n",
    "def _cites_to_table(cites):\n",
    "    table = []\n",
    "    for c in cites or []:\n",
    "        # ensure only scalars go into the dataframe\n",
    "        score = c.get(\"score\", \"\")\n",
    "        try:\n",
    "            score = round(float(score), 4)\n",
    "        except Exception:\n",
    "            pass\n",
    "        table.append([\n",
    "            c.get(\"#\", \"\"),\n",
    "            c.get(\"candidate\") or \"\",\n",
    "            c.get(\"role\") or \"\",\n",
    "            c.get(\"seniority\") or \"\",\n",
    "            c.get(\"location\") or \"\",\n",
    "            score,\n",
    "        ])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jmv36irebc-l"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4) FAISS Index Manager\n",
    "# ===============================\n",
    "class FaissStore:\n",
    "    \"\"\"\n",
    "    - FAISS index (IndexFlatIP) over L2-normalized vectors  => cosine similarity\n",
    "    - Python dict sidecar: id -> {\"document\": str, \"metadata\": {...}}\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, index_path: str, map_path: str):\n",
    "        self.dim = dim\n",
    "        self.index_path = index_path\n",
    "        self.map_path = map_path\n",
    "\n",
    "        self.id2row: Dict[str, int] = {}      # id -> row number in FAISS\n",
    "        self.row2id: List[str] = []           # row -> id\n",
    "        self.store: Dict[str, Dict[str, Any]] = {}  # id -> payload\n",
    "\n",
    "        if os.path.exists(index_path) and os.path.exists(map_path):\n",
    "            self._load()\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatIP(dim)\n",
    "\n",
    "    def add(self, ids: List[str], vectors: np.ndarray, documents: List[str], metadatas: List[Dict[str, Any]]):\n",
    "        assert vectors.shape[0] == len(ids) == len(documents) == len(metadatas)\n",
    "        # Add to FAISS\n",
    "        self.index.add(vectors)\n",
    "        # Update mappings\n",
    "        start_row = len(self.row2id)\n",
    "        for i, _id in enumerate(ids):\n",
    "            row = start_row + i\n",
    "            self.row2id.append(_id)\n",
    "            self.id2row[_id] = row\n",
    "            self.store[_id] = {\"document\": documents[i], \"metadata\": metadatas[i]}\n",
    "\n",
    "    def search(self, query_vectors: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Return (scores, row_indices). Scores are inner product (cosine).\"\"\"\n",
    "        scores, idxs = self.index.search(query_vectors, top_k)\n",
    "        return scores, idxs\n",
    "\n",
    "    def save(self):\n",
    "        faiss.write_index(self.index, self.index_path)\n",
    "        with open(self.map_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"row2id\": self.row2id,\n",
    "                \"store\": self.store\n",
    "            }, f, ensure_ascii=False)\n",
    "\n",
    "    def _load(self):\n",
    "        self.index = faiss.read_index(self.index_path)\n",
    "        with open(self.map_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        self.row2id = data.get(\"row2id\", [])\n",
    "        self.store  = data.get(\"store\", {})\n",
    "        self.id2row = {i: r for r, i in enumerate(self.row2id)}\n",
    "\n",
    "def do_qa_for_table(q, topk, sen, loc, role, kw):\n",
    "    answer, cites = qa_over_cvs(\n",
    "        question=q,\n",
    "        top_k_chunks=topk,\n",
    "        seniority=sen or \"\",\n",
    "        location=loc or \"\",\n",
    "        role_hint=role or \"\",\n",
    "        keyword=kw or \"\",\n",
    "    )\n",
    "    return answer, _cites_to_table(cites)\n",
    "\n",
    "\n",
    "# global store\n",
    "faiss_store = FaissStore(DIM, INDEX_PATH, MAP_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbZTq6ISZ-CC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 5) LLM: chunk + metadata (simple)\n",
    "# ===============================\n",
    "CHUNK_META_PROMPT = Template(\"\"\"You are helping HR index a candidate CV.\n",
    "Return STRICT JSON with this schema:\n",
    "{\n",
    "  \"candidate\": {\n",
    "    \"name\": string|null,\n",
    "    \"email\": string|null,\n",
    "    \"phone\": string|null,\n",
    "    \"location\": string|null,\n",
    "    \"years_experience\": number|null,\n",
    "    \"seniority\": \"Junior\"|\"Mid\"|\"Senior\"|null,\n",
    "    \"role\": string|null,\n",
    "    \"skills\": string[]\n",
    "  },\n",
    "  \"chunks\": [\n",
    "    {\n",
    "      \"text\": string,\n",
    "      \"section\": string|null\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Split into 4–20 coherent chunks; each chunk <= 800 characters.\n",
    "- Do not invent facts; if unknown, use null or [].\n",
    "- skills must be short lowercase tokens.\n",
    "- Output ONLY JSON.\n",
    "\n",
    "CV TEXT:\n",
    "\\\"\\\"\\\"$cv_text\\\"\\\"\\\"\"\"\")\n",
    "\n",
    "\n",
    "import re, json\n",
    "from string import Template\n",
    "\n",
    "# keep the same CHUNK_META_PROMPT you already have\n",
    "# CHUNK_META_PROMPT = Template(\"\"\" ... $cv_text ... \"\"\")\n",
    "\n",
    "NAME_RE  = re.compile(r\"(?im)^\\s*([A-Z][A-Za-z]+(?:\\s+[A-Z][A-Za-z'’\\-]+){0,3})\\s*$\")\n",
    "EMAIL_RE = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "PHONE_RE = re.compile(r\"(\\+?\\d[\\d\\-\\s\\(\\)]{7,})\")\n",
    "\n",
    "def _fallback_profile_from_text(t: str) -> dict:\n",
    "    lines = [ln.strip() for ln in (t or \"\").splitlines() if ln.strip()]\n",
    "    name = None\n",
    "    for ln in lines[:8]:\n",
    "        m = NAME_RE.match(ln)\n",
    "        if m:\n",
    "            name = m.group(1)\n",
    "            break\n",
    "    email = (EMAIL_RE.search(t) or [None])[0] if EMAIL_RE.search(t) else None\n",
    "    phone = (PHONE_RE.search(t) or [None])[0] if PHONE_RE.search(t) else None\n",
    "    return {\n",
    "        \"name\": name, \"email\": email, \"phone\": phone,\n",
    "        \"location\": None, \"years_experience\": None,\n",
    "        \"seniority\": None, \"role\": None, \"skills\": []\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def llm_chunk_and_metadata(cv_text: str) -> dict:\n",
    "    \"\"\"Stricter JSON mode + single retry; if still invalid, fall back to heuristics and multi-split.\"\"\"\n",
    "    prompt = CHUNK_META_PROMPT.substitute(cv_text=cv_text[:12000])\n",
    "    def _ask():\n",
    "        return client.chat.completions.create(\n",
    "            model=CHAT_MODEL,\n",
    "            messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "            temperature=0,\n",
    "            response_format={ \"type\": \"json_object\" }  # <-- JSON mode (supported by gpt-4o-mini)\n",
    "        )\n",
    "\n",
    "    raw = None\n",
    "    for _ in range(2):  # try twice\n",
    "        resp = _ask()\n",
    "        raw = resp.choices[0].message.content\n",
    "        try:\n",
    "            data = json.loads(raw)\n",
    "            # minimal sanity\n",
    "            if isinstance(data.get(\"chunks\"), list) and len(data[\"chunks\"]) >= 2:\n",
    "                return data\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # If we are here, LLM gave invalid JSON or too few chunks → heuristic fallback\n",
    "    profile = _fallback_profile_from_text(cv_text)\n",
    "    # simple multi-split fallback so you don't end up with a single chunk\n",
    "    text = \"\\n\".join(ln.strip() for ln in cv_text.splitlines())\n",
    "    chunks, size, overlap = [], 800, 120\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        j = min(len(text), i + size)\n",
    "        window = text[i:j]\n",
    "        last = max(window.rfind(\"\\n\\n\"), window.rfind(\". \"))\n",
    "        if last != -1 and (j - i) > 200:\n",
    "            j = i + last + (2 if window[last:last+2] == \"\\n\\n\" else 1)\n",
    "        chunk = text[i:j].strip()\n",
    "        if chunk:\n",
    "            chunks.append({\"text\": chunk, \"section\": None})\n",
    "        if j >= len(text): break\n",
    "        i = max(j - overlap, 0)\n",
    "        if i == j: break\n",
    "\n",
    "    return {\n",
    "        \"candidate\": profile,\n",
    "        \"chunks\": chunks if chunks else [{\"text\": cv_text[:800], \"section\": \"full_text\"}],\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Z48ZLiJcGp9"
   },
   "outputs": [],
   "source": [
    "===============================\n",
    "# 6) Ingest (multi-file)\n",
    "# ===============================\n",
    "def index_cvs(files, default_tags: str = \"cv\") -> str:\n",
    "    try:\n",
    "        file_paths = _normalize_file_list(files)\n",
    "        if not file_paths:\n",
    "            return _md_warn(\"No files selected.\")\n",
    "\n",
    "        tags = [t.strip() for t in (default_tags or \"\").split(\",\") if t.strip()]\n",
    "        total_chunks = 0\n",
    "        lines = [f\"### Index report ({len(file_paths)} file(s))\"]\n",
    "\n",
    "        for path in file_paths:\n",
    "            try:\n",
    "                file_name = os.path.basename(path)\n",
    "                text = read_any_text(path)\n",
    "                if not text.strip():\n",
    "                    lines.append(_md_warn(f\"{file_name}: empty text (scanned PDF?). Skipping.\"))\n",
    "                    continue\n",
    "\n",
    "                parsed = llm_chunk_and_metadata(text)\n",
    "                cand   = parsed.get(\"candidate\", {}) or {}\n",
    "                chunks = [c for c in parsed.get(\"chunks\", []) if c.get(\"text\")]\n",
    "                if not chunks:\n",
    "                    lines.append(_md_warn(f\"{file_name}: LLM returned 0 chunks. Skipping.\"))\n",
    "                    continue\n",
    "\n",
    "                candidate_id = str(uuid.uuid4())\n",
    "                docs   = [c[\"text\"][:2000] for c in chunks]  # safety cap\n",
    "                embs   = embed_texts(docs)\n",
    "                ids    = [f\"{candidate_id}-c{i}\" for i in range(len(docs))]\n",
    "                metas  = [{\n",
    "                    \"candidate_id\": candidate_id,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"section\": c.get(\"section\"),\n",
    "                    \"name\": cand.get(\"name\"),\n",
    "                    \"email\": cand.get(\"email\"),\n",
    "                    \"phone\": cand.get(\"phone\"),\n",
    "                    \"location\": cand.get(\"location\"),\n",
    "                    \"years_experience\": cand.get(\"years_experience\"),\n",
    "                    \"seniority\": cand.get(\"seniority\"),\n",
    "                    \"role\": cand.get(\"role\"),\n",
    "                    \"skills\": \", \".join(cand.get(\"skills\") or []),   # store as string in sidecar\n",
    "                    \"tags\": \", \".join(tags),\n",
    "                    \"doc_type\": \"cv\",\n",
    "                } for c in chunks]\n",
    "\n",
    "                # Add to FAISS store (no type restriction headaches)\n",
    "                faiss_store.add(ids, embs, docs, metas)\n",
    "                total_chunks += len(docs)\n",
    "                lines.append(_md_ok(f\"{file_name}: indexed {len(docs)} chunk(s). Candidate: {cand.get('name') or 'Unknown'}\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                lines.append(_md_err(f\"{os.path.basename(path)}: {e}\"))\n",
    "                lines.append(\"```text\\n\" + traceback.format_exc() + \"\\n```\")\n",
    "\n",
    "        if total_chunks == 0:\n",
    "            lines.append(_md_warn(\"No chunks were added. Check errors above (API key, PDF text, etc.).\"))\n",
    "        else:\n",
    "            faiss_store.save()\n",
    "            lines.append(_md_ok(f\"Saved index with total chunks: {len(faiss_store.row2id)}\"))\n",
    "        return \"\\n\\n\".join(lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        return _md_err(f\"Top-level error: {e}\") + \"\\n\\n```text\\n\" + traceback.format_exc() + \"\\n```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CM0phFF2aYId"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 7) Search (JD match) — group by candidate\n",
    "# ===============================\n",
    "def search_candidates(jd_text: str,\n",
    "                      top_k_candidates: int = 10,\n",
    "                      seniority: str = \"\", location: str = \"\", role_hint: str = \"\") -> List[Dict[str, Any]]:\n",
    "    if not jd_text.strip():\n",
    "        return []\n",
    "\n",
    "    qvec = embed_texts([jd_text])[0].reshape(1, -1)\n",
    "    scores, rows = faiss_store.search(qvec, top_k=200)   # pull wide\n",
    "    scores, rows = scores[0], rows[0]\n",
    "\n",
    "    # Gather hits, apply light metadata filters from sidecar\n",
    "    by_cand: Dict[str, Dict[str, Any]] = {}\n",
    "    for s, r in zip(scores, rows):\n",
    "        if r == -1: continue\n",
    "        _id = faiss_store.row2id[r]\n",
    "        rec = faiss_store.store[_id]\n",
    "        md  = rec[\"metadata\"]\n",
    "\n",
    "        if seniority and (md.get(\"seniority\") or \"\") != seniority:\n",
    "            continue\n",
    "        if location and (md.get(\"location\") or \"\") != location:\n",
    "            continue\n",
    "        if role_hint and (md.get(\"role\") or \"\") != role_hint:\n",
    "            continue\n",
    "\n",
    "        cid = md.get(\"candidate_id\", \"unknown\")\n",
    "        if cid not in by_cand:\n",
    "            by_cand[cid] = {\n",
    "                \"Score\": 0.0,\n",
    "                \"Name\": md.get(\"name\"),\n",
    "                \"Email\": md.get(\"email\"),\n",
    "                \"Phone\": md.get(\"phone\"),\n",
    "                \"Location\": md.get(\"location\"),\n",
    "                \"YearsExp\": md.get(\"years_experience\"),\n",
    "                \"Seniority\": md.get(\"seniority\"),\n",
    "                \"Role\": md.get(\"role\"),\n",
    "                \"Skills\": md.get(\"skills\"),\n",
    "                \"Snippets\": []\n",
    "            }\n",
    "        by_cand[cid][\"Score\"] = max(by_cand[cid][\"Score\"], float(s))  # use best (max) cosine\n",
    "        if len(by_cand[cid][\"Snippets\"]) < 2:\n",
    "            doc = rec[\"document\"]\n",
    "            by_cand[cid][\"Snippets\"].append(doc[:220] + (\"...\" if len(doc) > 220 else \"\"))\n",
    "\n",
    "    rows_out = list(by_cand.values())\n",
    "    rows_out.sort(key=lambda r: r[\"Score\"], reverse=True)\n",
    "    for r in rows_out:\n",
    "        r[\"Score\"] = round(r[\"Score\"], 4)\n",
    "    return rows_out[:top_k_candidates]\n",
    "\n",
    "# ===============================\n",
    "# 8) Q&A (grounded) with optional keyword prefilter\n",
    "# ===============================\n",
    "# --- Q&A: dedupe by candidate and flatten output ---\n",
    "\n",
    "RAG_SYSTEM = (\n",
    "    \"You are an HR assistant. Answer ONLY from the provided CONTEXT.\\n\"\n",
    "    \"If the answer is not present, say you don't know. Be concise.\"\n",
    ")\n",
    "\n",
    "def qa_over_cvs_dedup(question: str, top_k_chunks: int = 6,\n",
    "                      seniority: str = \"\", location: str = \"\", role_hint: str = \"\", keyword: str = \"\"):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\", []\n",
    "\n",
    "    # 1) Retrieve (overfetch) then filter\n",
    "    qvec = embed_texts([question])[0].reshape(1, -1)\n",
    "    scores, rows = faiss_store.search(qvec, top_k=top_k_chunks * 6)\n",
    "    scores, rows = scores[0], rows[0]\n",
    "\n",
    "    # 2) Build context (keep up to top_k_chunks unique chunks after filters)\n",
    "    ctx_parts = []\n",
    "    per_candidate = {}  # candidate_id -> {score, name, role, seniority, location, snippet}\n",
    "    kept = 0\n",
    "    for s, r in zip(scores, rows):\n",
    "        if r == -1:\n",
    "            continue\n",
    "        _id = faiss_store.row2id[r]\n",
    "        rec = faiss_store.store[_id]\n",
    "        md  = rec[\"metadata\"]\n",
    "        doc = rec[\"document\"]\n",
    "\n",
    "        # filters\n",
    "        if seniority and (md.get(\"seniority\") or \"\") != seniority:   continue\n",
    "        if location  and (md.get(\"location\")  or \"\") != location:    continue\n",
    "        if role_hint and (md.get(\"role\")      or \"\") != role_hint:   continue\n",
    "        if keyword   and (keyword.lower() not in (doc or \"\").lower()): continue\n",
    "\n",
    "        # add to LLM context until limit\n",
    "        if kept < top_k_chunks:\n",
    "            label = f\"{md.get('name') or md.get('file_name')} | {md.get('section')}\"\n",
    "            ctx_parts.append(f\"[{kept+1} | {label}]\\n{doc}\")\n",
    "            kept += 1\n",
    "\n",
    "        # aggregate per candidate (best score + first good snippet)\n",
    "        cid = md.get(\"candidate_id\") or _id\n",
    "        current = per_candidate.get(cid, {\n",
    "            \"candidate\": md.get(\"name\") or md.get(\"file_name\"),\n",
    "            \"role\": md.get(\"role\") or \"\",\n",
    "            \"seniority\": md.get(\"seniority\") or \"\",\n",
    "            \"location\": md.get(\"location\") or \"\",\n",
    "            \"score\": 0.0,\n",
    "            \"snippet\": \"\"\n",
    "        })\n",
    "        if float(s) > current[\"score\"]:\n",
    "            current[\"score\"] = float(s)\n",
    "            # keep a short snippet for the table\n",
    "            current[\"snippet\"] = (doc[:220] + (\"...\" if len(doc) > 220 else \"\"))\n",
    "        per_candidate[cid] = current\n",
    "\n",
    "    if not ctx_parts:\n",
    "        return \"No relevant information found.\", []\n",
    "\n",
    "    # 3) Ask LLM with grounded context\n",
    "    ctx = \"\\n\\n\".join(ctx_parts)\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\": RAG_SYSTEM},\n",
    "        {\"role\":\"user\",\"content\": f\"QUESTION: {question}\\n\\nCONTEXT:\\n{ctx}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=CHAT_MODEL, messages=messages, temperature=0)\n",
    "    answer = resp.choices[0].message.content\n",
    "\n",
    "    # 4) Build a clean, flat table (no objects)\n",
    "    rows = []\n",
    "    for i, (_, c) in enumerate(sorted(per_candidate.items(), key=lambda kv: kv[1][\"score\"], reverse=True), start=1):\n",
    "        rows.append([\n",
    "            i,\n",
    "            c[\"candidate\"],\n",
    "            c[\"role\"],\n",
    "            c[\"seniority\"],\n",
    "            c[\"location\"],\n",
    "            round(c[\"score\"], 4),\n",
    "            c[\"snippet\"]\n",
    "        ])\n",
    "    return answer, rows\n",
    "\n",
    "QA_HEADERS = [\"#\", \"Candidate\", \"Role\", \"Seniority\", \"Location\", \"Score\", \"Snippet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "efhKKRG4dDdJ",
    "outputId": "03778400-bf0c-424f-c2e0-07732cba463b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://0eaf5e98a4cb8ad253.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0eaf5e98a4cb8ad253.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gr.Blocks(title=\"HR CV Search (FAISS) — LLM Chunk + Metadata\") as app:\n",
    "    gr.Markdown(\"### HR CV RAG (FAISS): Upload CVs → LLM chunks & metadata → Search/JD → Q&A\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Ingest\"):\n",
    "            up = gr.File(file_count=\"multiple\", type=\"filepath\", label=\"Upload CVs (PDF/TXT)\")\n",
    "            tags = gr.Textbox(value=\"cv\", label=\"Default tags (optional)\")  # saved in sidecar only\n",
    "            go = gr.Button(\"Index\", variant=\"primary\")\n",
    "            out = gr.Markdown()\n",
    "            go.click(index_cvs, inputs=[up, tags], outputs=out)\n",
    "\n",
    "        with gr.TabItem(\"Search (JD match)\"):\n",
    "            jd   = gr.Textbox(lines=6, label=\"Job Description\")\n",
    "            with gr.Row():\n",
    "                sen = gr.Dropdown(choices=[\"\",\"Junior\",\"Mid\",\"Senior\"], value=\"\", label=\"Seniority\")\n",
    "                loc = gr.Textbox(label=\"Location\")\n",
    "                role = gr.Textbox(label=\"Role\")\n",
    "            topk = gr.Slider(1, 30, value=10, step=1, label=\"Top-K candidates\")\n",
    "            btn  = gr.Button(\"Search\", variant=\"primary\")\n",
    "\n",
    "            tbl  = gr.Dataframe(\n",
    "                headers=TABLE_HEADERS,\n",
    "                wrap=True,\n",
    "                interactive=False,\n",
    "                row_count=(0, \"dynamic\"),\n",
    "                col_count=(len(TABLE_HEADERS), \"fixed\"),\n",
    "            )\n",
    "\n",
    "        btn.click(do_search_for_table, inputs=[jd, topk, sen, loc, role], outputs=tbl)\n",
    "\n",
    "\n",
    "        # --- Replace only the Q&A tab in your UI block ---\n",
    "        with gr.TabItem(\"Q&A\"):\n",
    "            q = gr.Textbox(lines=2, label=\"Question\", placeholder=\"e.g., Who has hands-on RAG experience?\")\n",
    "            with gr.Row():\n",
    "                topk_c = gr.Slider(1, 12, value=6, step=1, label=\"Top-K chunks (for context)\")\n",
    "                sen2 = gr.Dropdown(choices=[\"\",\"Junior\",\"Mid\",\"Senior\"], value=\"\", label=\"Seniority\")\n",
    "                loc2 = gr.Textbox(label=\"Location\")\n",
    "                role2 = gr.Textbox(label=\"Role\")\n",
    "            kw = gr.Textbox(label=\"Keyword prefilter (optional)\", placeholder=\"e.g. Airflow\")\n",
    "\n",
    "            ask = gr.Button(\"Ask\", variant=\"primary\")\n",
    "            ans = gr.Markdown()\n",
    "            cites = gr.Dataframe(headers=QA_HEADERS, wrap=True, interactive=False,\n",
    "                              row_count=(0, \"dynamic\"), col_count=(len(QA_HEADERS), \"fixed\"))\n",
    "\n",
    "        ask.click(qa_over_cvs_dedup,\n",
    "                  inputs=[q, topk_c, sen2, loc2, role2, kw],\n",
    "                  outputs=[ans, cites])\n",
    "\n",
    "\n",
    "\n",
    "app.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
